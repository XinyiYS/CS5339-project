{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'flags'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f7ebdfc6f245>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcleverhans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcleverhans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossEntropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcleverhans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'flags'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This tutorial shows how to generate adversarial examples using FGSM\n",
    "and train a model using adversarial training with TensorFlow.\n",
    "It is very similar to mnist_tutorial_keras_tf.py, which does the same\n",
    "thing but with a dependence on keras.\n",
    "The original paper can be found at:\n",
    "https://arxiv.org/abs/1412.6572\n",
    "\"\"\"\n",
    "# pylint: disable=missing-docstring\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from cleverhans.compat import flags\n",
    "from cleverhans.loss import CrossEntropy\n",
    "from cleverhans.dataset import MNIST\n",
    "from cleverhans.utils_tf import model_eval\n",
    "from cleverhans.train import train\n",
    "from cleverhans.attacks import FastGradientMethod\n",
    "from cleverhans.utils import AccuracyReport, set_log_level\n",
    "from cleverhans.model_zoo.basic_cnn import ModelBasicCNN\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "NB_EPOCHS = 6\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "CLEAN_TRAIN = True\n",
    "BACKPROP_THROUGH_ATTACK = False\n",
    "NB_FILTERS = 64\n",
    "\n",
    "\n",
    "def mnist_tutorial(train_start=0, train_end=60000, test_start=0,\n",
    "                   test_end=10000, nb_epochs=NB_EPOCHS, batch_size=BATCH_SIZE,\n",
    "                   learning_rate=LEARNING_RATE,\n",
    "                   clean_train=CLEAN_TRAIN,\n",
    "                   testing=False,\n",
    "                   backprop_through_attack=BACKPROP_THROUGH_ATTACK,\n",
    "                   nb_filters=NB_FILTERS, num_threads=None,\n",
    "                   label_smoothing=0.1):\n",
    "  \"\"\"\n",
    "  MNIST cleverhans tutorial\n",
    "  :param train_start: index of first training set example\n",
    "  :param train_end: index of last training set example\n",
    "  :param test_start: index of first test set example\n",
    "  :param test_end: index of last test set example\n",
    "  :param nb_epochs: number of epochs to train model\n",
    "  :param batch_size: size of training batches\n",
    "  :param learning_rate: learning rate for training\n",
    "  :param clean_train: perform normal training on clean examples only\n",
    "                      before performing adversarial training.\n",
    "  :param testing: if true, complete an AccuracyReport for unit tests\n",
    "                  to verify that performance is adequate\n",
    "  :param backprop_through_attack: If True, backprop through adversarial\n",
    "                                  example construction process during\n",
    "                                  adversarial training.\n",
    "  :param label_smoothing: float, amount of label smoothing for cross entropy\n",
    "  :return: an AccuracyReport object\n",
    "  \"\"\"\n",
    "\n",
    "  # Object used to keep track of (and return) key accuracies\n",
    "  report = AccuracyReport()\n",
    "\n",
    "  # Set TF random seed to improve reproducibility\n",
    "  tf.set_random_seed(1234)\n",
    "\n",
    "  # Set logging level to see debug information\n",
    "  set_log_level(logging.DEBUG)\n",
    "\n",
    "  # Create TF session\n",
    "  if num_threads:\n",
    "    config_args = dict(intra_op_parallelism_threads=1)\n",
    "  else:\n",
    "    config_args = {}\n",
    "  sess = tf.Session(config=tf.ConfigProto(**config_args))\n",
    "\n",
    "  # Get MNIST data\n",
    "  mnist = MNIST(train_start=train_start, train_end=train_end,\n",
    "                test_start=test_start, test_end=test_end)\n",
    "  x_train, y_train = mnist.get_set('train')\n",
    "  x_test, y_test = mnist.get_set('test')\n",
    "\n",
    "  # Use Image Parameters\n",
    "  img_rows, img_cols, nchannels = x_train.shape[1:4]\n",
    "  nb_classes = y_train.shape[1]\n",
    "\n",
    "  # Define input TF placeholder\n",
    "  x = tf.placeholder(tf.float32, shape=(None, img_rows, img_cols,\n",
    "                                        nchannels))\n",
    "  y = tf.placeholder(tf.float32, shape=(None, nb_classes))\n",
    "\n",
    "  # Train an MNIST model\n",
    "  train_params = {\n",
    "      'nb_epochs': nb_epochs,\n",
    "      'batch_size': batch_size,\n",
    "      'learning_rate': learning_rate\n",
    "  }\n",
    "  eval_params = {'batch_size': batch_size}\n",
    "  fgsm_params = {\n",
    "      'eps': 0.3,\n",
    "      'clip_min': 0.,\n",
    "      'clip_max': 1.\n",
    "  }\n",
    "  rng = np.random.RandomState([2017, 8, 30])\n",
    "\n",
    "  def do_eval(preds, x_set, y_set, report_key, is_adv=None):\n",
    "    acc = model_eval(sess, x, y, preds, x_set, y_set, args=eval_params)\n",
    "    setattr(report, report_key, acc)\n",
    "    if is_adv is None:\n",
    "      report_text = None\n",
    "    elif is_adv:\n",
    "      report_text = 'adversarial'\n",
    "    else:\n",
    "      report_text = 'legitimate'\n",
    "    if report_text:\n",
    "      print('Test accuracy on %s examples: %0.4f' % (report_text, acc))\n",
    "\n",
    "  if clean_train:\n",
    "    model = ModelBasicCNN('model1', nb_classes, nb_filters)\n",
    "    preds = model.get_logits(x)\n",
    "    loss = CrossEntropy(model, smoothing=label_smoothing)\n",
    "\n",
    "    def evaluate():\n",
    "      do_eval(preds, x_test, y_test, 'clean_train_clean_eval', False)\n",
    "\n",
    "    train(sess, loss, x_train, y_train, evaluate=evaluate,\n",
    "          args=train_params, rng=rng, var_list=model.get_params())\n",
    "\n",
    "    # Calculate training error\n",
    "    if testing:\n",
    "      do_eval(preds, x_train, y_train, 'train_clean_train_clean_eval')\n",
    "\n",
    "    # Initialize the Fast Gradient Sign Method (FGSM) attack object and\n",
    "    # graph\n",
    "    fgsm = FastGradientMethod(model, sess=sess)\n",
    "    adv_x = fgsm.generate(x, **fgsm_params)\n",
    "    preds_adv = model.get_logits(adv_x)\n",
    "\n",
    "    # Evaluate the accuracy of the MNIST model on adversarial examples\n",
    "    do_eval(preds_adv, x_test, y_test, 'clean_train_adv_eval', True)\n",
    "\n",
    "    # Calculate training error\n",
    "    if testing:\n",
    "      do_eval(preds_adv, x_train, y_train, 'train_clean_train_adv_eval')\n",
    "\n",
    "    print('Repeating the process, using adversarial training')\n",
    "\n",
    "  # Create a new model and train it to be robust to FastGradientMethod\n",
    "  model2 = ModelBasicCNN('model2', nb_classes, nb_filters)\n",
    "  fgsm2 = FastGradientMethod(model2, sess=sess)\n",
    "\n",
    "  def attack(x):\n",
    "    return fgsm2.generate(x, **fgsm_params)\n",
    "\n",
    "  loss2 = CrossEntropy(model2, smoothing=label_smoothing, attack=attack)\n",
    "  preds2 = model2.get_logits(x)\n",
    "  adv_x2 = attack(x)\n",
    "\n",
    "  if not backprop_through_attack:\n",
    "    # For the fgsm attack used in this tutorial, the attack has zero\n",
    "    # gradient so enabling this flag does not change the gradient.\n",
    "    # For some other attacks, enabling this flag increases the cost of\n",
    "    # training, but gives the defender the ability to anticipate how\n",
    "    # the atacker will change their strategy in response to updates to\n",
    "    # the defender's parameters.\n",
    "    adv_x2 = tf.stop_gradient(adv_x2)\n",
    "  preds2_adv = model2.get_logits(adv_x2)\n",
    "\n",
    "  def evaluate2():\n",
    "    # Accuracy of adversarially trained model on legitimate test inputs\n",
    "    do_eval(preds2, x_test, y_test, 'adv_train_clean_eval', False)\n",
    "    # Accuracy of the adversarially trained model on adversarial examples\n",
    "    do_eval(preds2_adv, x_test, y_test, 'adv_train_adv_eval', True)\n",
    "\n",
    "  # Perform and evaluate adversarial training\n",
    "  train(sess, loss2, x_train, y_train, evaluate=evaluate2,\n",
    "        args=train_params, rng=rng, var_list=model2.get_params())\n",
    "\n",
    "  # Calculate training errors\n",
    "  if testing:\n",
    "    do_eval(preds2, x_train, y_train, 'train_adv_train_clean_eval')\n",
    "    do_eval(preds2_adv, x_train, y_train, 'train_adv_train_adv_eval')\n",
    "\n",
    "  return report\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "  \"\"\"\n",
    "  Run the tutorial using command line flags.\n",
    "  \"\"\"\n",
    "  from cleverhans_tutorials import check_installation\n",
    "  check_installation(__file__)\n",
    "\n",
    "  mnist_tutorial(nb_epochs=FLAGS.nb_epochs, batch_size=FLAGS.batch_size,\n",
    "                 learning_rate=FLAGS.learning_rate,\n",
    "                 clean_train=FLAGS.clean_train,\n",
    "                 backprop_through_attack=FLAGS.backprop_through_attack,\n",
    "                 nb_filters=FLAGS.nb_filters)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  flags.DEFINE_integer('nb_filters', NB_FILTERS,\n",
    "                       'Model size multiplier')\n",
    "  flags.DEFINE_integer('nb_epochs', NB_EPOCHS,\n",
    "                       'Number of epochs to train model')\n",
    "  flags.DEFINE_integer('batch_size', BATCH_SIZE,\n",
    "                       'Size of training batches')\n",
    "  flags.DEFINE_float('learning_rate', LEARNING_RATE,\n",
    "                     'Learning rate for training')\n",
    "  flags.DEFINE_bool('clean_train', CLEAN_TRAIN, 'Train on clean examples')\n",
    "  flags.DEFINE_bool('backprop_through_attack', BACKPROP_THROUGH_ATTACK,\n",
    "                    ('If True, backprop through adversarial example '\n",
    "                     'construction process during adversarial training'))\n",
    "\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
